import requests
import json
import os
import hashlib
from datetime import datetime
from bs4 import BeautifulSoup
from google.oauth2 import service_account
from google.auth.transport.requests import Request
from urllib.parse import urljoin

# ================== CONFIG ==================

HEADERS = {
â€œUser-Agentâ€: â€œMozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36â€,
â€œAcceptâ€: â€œtext/html,application/xhtml+xml,application/xml;q=0.9,/;q=0.8â€,
â€œAccept-Languageâ€: â€œtr-TR,tr;q=0.9,en;q=0.8â€,
â€œAccept-Encodingâ€: â€œgzip, deflate, brâ€,
â€œConnectionâ€: â€œkeep-aliveâ€,
}

PROJECT_ID = os.getenv(â€œFIREBASE_PROJECT_IDâ€) or â€œnaberr-6f4e4â€
SERVICE_ACCOUNT_FILE = â€œservice-account.jsonâ€
DATA_FILE = â€œsent_news.jsonâ€

# ğŸ”‘ SINAVLAR

EXAMS = [
â€œyksâ€, â€œtytâ€, â€œaytâ€, â€œydtâ€,
â€œkpssâ€, â€œalesâ€, â€œdgsâ€, â€œmsÃ¼â€,
â€œydsâ€, â€œe-ydsâ€, â€œtusâ€, â€œydusâ€,
â€œekpssâ€, â€œdhbtâ€, â€œstsâ€, â€œmbstsâ€,
â€œagsâ€, â€œhmbstsâ€
]

# ğŸ¯ RESMÃ DUYURU SAYFALARI

SOURCES = [
(â€œhttps://www.osym.gov.tr/TR,33759/2026.htmlâ€, â€œÃ–SYMâ€),
(â€œhttps://www.osym.gov.tr/TR,6/duyurular.htmlâ€, â€œÃ–SYMâ€),      # Yedek Ã–SYM sayfasÄ±
(â€œhttps://www.meb.gov.tr/meb_duyuruindex.phpâ€, â€œMEBâ€),
]

# Resmi Gazete ayrÄ± bir fonksiyonla taranÄ±yor (Ã¶zel URL yapÄ±sÄ±)

RESMI_GAZETE_BASE = â€œhttps://www.resmigazete.gov.trâ€

# ================== TOKEN CACHE ==================

_cached_token = None
_token_expiry = None

def get_access_token():
global _cached_token, _token_expiry


now = datetime.utcnow()

if _cached_token and _token_expiry and now < _token_expiry:
    return _cached_token

credentials = service_account.Credentials.from_service_account_file(
    SERVICE_ACCOUNT_FILE,
    scopes=["https://www.googleapis.com/auth/firebase.messaging"]
)
credentials.refresh(Request())

_cached_token = credentials.token
# Token'Ä± 50 dakika geÃ§erli say (Google 60dk veriyor)
from datetime import timedelta
_token_expiry = now + timedelta(minutes=50)

return _cached_token


# ================== FIREBASE ==================

def send_fcm(topic, data):
print(fâ€ğŸ“£ FCM gÃ¶nderiliyor â†’ topic: {topic} | baÅŸlÄ±k: {data[â€˜titleâ€™]}â€)


url = f"https://fcm.googleapis.com/v1/projects/{PROJECT_ID}/messages:send"

payload = {
    "message": {
        "topic": topic,
        "notification": {
            "title": data["title"],
            "body": f"{data['examType']} â€¢ Yeni duyuru"
        },
        "data": {
            "examType": data["examType"],
            "url": data["url"],
            "source": data["source"]
        },
        "android": {
            "priority": "HIGH",
            "notification": {
                "sound": "default",
                "channel_id": "exam_notifications"
            }
        },
        "apns": {
            "payload": {
                "aps": {
                    "sound": "default"
                }
            }
        }
    }
}

try:
    res = requests.post(
        url,
        headers={
            "Authorization": f"Bearer {get_access_token()}",
            "Content-Type": "application/json"
        },
        json=payload,
        timeout=15
    )

    if res.status_code == 200:
        print(f"âœ… Bildirim gÃ¶nderildi: {data['title']}")
    else:
        print(f"âŒ FCM HATA [{res.status_code}]: {res.text}")

except Exception as e:
    print(f"âŒ FCM isteÄŸi baÅŸarÄ±sÄ±z: {e}")


# ================== SCRAPER ==================

def generate_news_id(title, source):
raw = fâ€{title.lower().strip()}|{source.lower().strip()}â€
return hashlib.sha256(raw.encode(â€œutf-8â€)).hexdigest()

def detect_exam_type(title):
t = title.lower()
for e in EXAMS:
if e in t:
return e.upper()
return â€œGENELâ€

def is_relevant_news(title):
t = title.lower()
return any(e in t for e in EXAMS)

def scrape_site(url, source):
results = []


print(f"  ğŸŒ BaÄŸlanÄ±lÄ±yor: {url}")

try:
    session = requests.Session()
    session.headers.update(HEADERS)

    r = session.get(url, timeout=20)
    r.raise_for_status()
    r.encoding = r.apparent_encoding  # TÃ¼rkÃ§e karakter dÃ¼zeltme

    soup = BeautifulSoup(r.text, "html.parser")

    # TÃ¼m linkleri tara (limit kaldÄ±rÄ±ldÄ±)
    links = soup.find_all("a", href=True)
    print(f"  ğŸ“„ Toplam link bulundu: {len(links)}")

    found = 0
    for a in links:
        title = " ".join(a.get_text().split()).strip()
        link = a.get("href", "").strip()

        # BoÅŸ veya Ã§ok kÄ±sa baÅŸlÄ±klarÄ± atla
        if not title or len(title) < 15:
            continue

        # BoÅŸ veya javascript: linklerini atla
        if not link or link.startswith("javascript:") or link == "#":
            continue

        # Tam URL yap
        link = urljoin(url, link)

        if is_relevant_news(title):
            news_id = generate_news_id(title, source)
            results.append({
                "id": news_id,
                "title": title,
                "link": link,
                "source": source,
                "createdAt": datetime.utcnow().isoformat()
            })
            found += 1
            print(f"  âœ” Haber bulundu: {title[:80]}")

    print(f"  ğŸ“Š {source}: {found} ilgili haber bulundu")

except requests.exceptions.ConnectionError:
    print(f"  âŒ {source} baÄŸlantÄ± hatasÄ±: Siteye ulaÅŸÄ±lamÄ±yor")
except requests.exceptions.Timeout:
    print(f"  âŒ {source} zaman aÅŸÄ±mÄ±: Site yanÄ±t vermiyor")
except requests.exceptions.HTTPError as e:
    print(f"  âŒ {source} HTTP hatasÄ±: {e}")
except Exception as e:
    print(f"  âŒ {source} beklenmeyen hata: {e}")

return results


# ================== RESMÄ° GAZETE SCRAPER ==================

def scrape_resmi_gazete():
â€œâ€â€
Resmi Gazeteâ€™nin gÃ¼ncel sayÄ±sÄ±nÄ± tarar.
URL formatÄ±: https://www.resmigazete.gov.tr/eskiler/YYYY/MM/YYYYMMDD.htm
â€œâ€â€
results = []
source = â€œResmÃ® Gazeteâ€


# BugÃ¼n ve dÃ¼n iÃ§in dene (hafta sonu yayÄ±mlanmayabilir)
from datetime import timedelta
dates_to_try = [
    datetime.utcnow(),
    datetime.utcnow() - timedelta(days=1),
    datetime.utcnow() - timedelta(days=2),
    datetime.utcnow() - timedelta(days=3),
]

for dt in dates_to_try:
    year = dt.strftime("%Y")
    month = dt.strftime("%m")
    day = dt.strftime("%Y%m%d")
    url = f"{RESMI_GAZETE_BASE}/eskiler/{year}/{month}/{day}.htm"

    print(f"  ğŸŒ ResmÃ® Gazete deneniyor: {url}")

    try:
        session = requests.Session()
        session.headers.update(HEADERS)

        r = session.get(url, timeout=20)

        if r.status_code == 404:
            print(f"  âš ï¸ {day} sayÄ±sÄ± bulunamadÄ± (muhtemelen yayÄ±mlanmadÄ±), Ã¶nceki gÃ¼n deneniyor...")
            continue

        r.raise_for_status()
        r.encoding = "windows-1254"  # Resmi Gazete genellikle bu encoding kullanÄ±r

        soup = BeautifulSoup(r.text, "html.parser")
        links = soup.find_all("a", href=True)
        print(f"  ğŸ“„ ResmÃ® Gazete toplam link: {len(links)}")

        found = 0
        for a in links:
            title = " ".join(a.get_text().split()).strip()
            link = a.get("href", "").strip()

            if not title or len(title) < 15:
                continue
            if not link or link.startswith("javascript:") or link == "#":
                continue

            link = urljoin(url, link)

            if is_relevant_news(title):
                news_id = generate_news_id(title, source)
                results.append({
                    "id": news_id,
                    "title": title,
                    "link": link,
                    "source": source,
                    "createdAt": datetime.utcnow().isoformat()
                })
                found += 1
                print(f"  âœ” ResmÃ® Gazete haberi: {title[:80]}")

        print(f"  ğŸ“Š ResmÃ® Gazete {day}: {found} ilgili haber bulundu")

        # BaÅŸarÄ±lÄ± olduysa dÃ¶ngÃ¼den Ã§Ä±k
        break

    except requests.exceptions.ConnectionError:
        print(f"  âŒ ResmÃ® Gazete baÄŸlantÄ± hatasÄ±")
        break
    except requests.exceptions.Timeout:
        print(f"  âŒ ResmÃ® Gazete zaman aÅŸÄ±mÄ±")
        break
    except Exception as e:
        print(f"  âŒ ResmÃ® Gazete beklenmeyen hata: {e}")
        break

return results


# ================== MAIN ==================

def main():
print(â€=â€ * 50)
print(â€œğŸš€ SÄ±nav Duyuru Botu BaÅŸladÄ±â€)
print(fâ€â° Zaman: {datetime.utcnow().isoformat()}â€)
print(â€=â€ * 50)


# GÃ¶nderilmiÅŸ haberleri yÃ¼kle
sent_ids = set()
if os.path.exists(DATA_FILE):
    try:
        with open(DATA_FILE, "r", encoding="utf-8") as f:
            sent_ids = set(json.load(f))
        print(f"ğŸ“‚ Daha Ã¶nce gÃ¶nderilmiÅŸ {len(sent_ids)} haber yÃ¼klendi")
    except Exception as e:
        print(f"âš ï¸ sent_news.json okunamadÄ±, sÄ±fÄ±rdan baÅŸlÄ±yor: {e}")
else:
    print("ğŸ“‚ sent_news.json bulunamadÄ±, yeni oluÅŸturulacak")

new_ids = []
all_news = []

# TÃ¼m siteleri tara
seen_urls = set()  # AynÄ± Ã–SYM URL'sini iki kez tarama
for url, source in SOURCES:
    if url in seen_urls:
        continue
    seen_urls.add(url)

    print(f"\nğŸ” {source} taranÄ±yor...")
    news = scrape_site(url, source)
    all_news.extend(news)

# ResmÃ® Gazete'yi tara
print(f"\nğŸ” ResmÃ® Gazete taranÄ±yor...")
rg_news = scrape_resmi_gazete()
all_news.extend(rg_news)

print(f"\nğŸ“‹ Toplam bulunan haber: {len(all_news)}")

# Yeni haberleri gÃ¶nder
for item in all_news:
    if item["id"] in sent_ids:
        print(f"â­ï¸ Zaten gÃ¶nderildi, atlanÄ±yor: {item['title'][:60]}")
        continue

    exam_type = detect_exam_type(item["title"])

    send_fcm(
        topic=exam_type.lower(),
        data={
            "title": item["title"],
            "examType": exam_type,
            "url": item["link"],
            "source": item["source"]
        }
    )

    sent_ids.add(item["id"])
    new_ids.append(item["id"])

# Kaydet
if new_ids:
    try:
        with open(DATA_FILE, "w", encoding="utf-8") as f:
            json.dump(list(sent_ids), f, indent=2, ensure_ascii=False)
        print(f"\nğŸ’¾ sent_news.json gÃ¼ncellendi")
    except Exception as e:
        print(f"âš ï¸ Kaydetme hatasÄ±: {e}")

print("\n" + "=" * 50)
print(f"âœ… TamamlandÄ±! Yeni gÃ¶nderilen bildirim: {len(new_ids)}")
print("=" * 50)


if *__name__* == â€œ*__main__*â€:
main()
